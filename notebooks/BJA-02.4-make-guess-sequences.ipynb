{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-ticks')\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "import seaborn as sns\n",
    "\n",
    "# Allow imports from project dir\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.Wordle import Wordle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_lfl(word_list, bow, colname):\n",
    "    \"\"\"    \n",
    "    Calculate Likelihood of word from Letter Frequency given in a bag of words 5x26 matrix \n",
    "        see BJA-01.2-make-word-lists\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for word in word_list:\n",
    "        likelihood = 1\n",
    "        for i, letter in enumerate(word):\n",
    "            likelihood *= bow[str(i)][letter]\n",
    "        result.append((word, likelihood))\n",
    "    return pd.DataFrame(sorted(result, key=lambda x: x[1], reverse=True), columns=['w', colname]).set_index('w') \n",
    "    \n",
    "    \n",
    "def contains_any(word1, word2, max_repeat=0):\n",
    "    \"\"\" Check whether word1 contains any of the letters in word2. \"\"\"\n",
    "#     set_ = set(x for x in str2)\n",
    "#     return 1 in [c in str1 for c in set_]\n",
    "    return any(letter in word1 for letter in word2)\n",
    "\n",
    "\n",
    "def total_green_letter_prob(word, bow):\n",
    "    # Finds total probability that you will get at least 1 green in any position by testing this word\n",
    "    prob = 0\n",
    "    for i, letter in enumerate(word):\n",
    "        prob += bow[str(i)][letter]\n",
    "    return prob\n",
    "\n",
    "def prob_of_2_greens(word):\n",
    "    # How do you calculate this? \n",
    "    pass\n",
    "\n",
    "def total_yellow_letter_prob(word):\n",
    "    prob = 0 \n",
    "    \n",
    "    \n",
    "# Allow for repeat letters with max_repeats\n",
    "def contains_repeats(word, search, max_repeats):\n",
    "    s = set(letter for letter in search)\n",
    "    repeats = 0\n",
    "    for letter in word:\n",
    "        if letter in s:\n",
    "            repeats += 1\n",
    "    if repeats <= max_repeats:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word list with frequencies\n",
    "wl = pd.read_csv(r'..\\data\\processed\\wordle_google_freq_word_list.csv', index_col=0)\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "# Read in  Bags of Letters matrices (AKA bag-of-words = bow) \n",
    "with open(r'..\\data\\processed\\bags_of_words.json', 'r') as infile:\n",
    "    all_bow = json.load(infile, parse_int=False)\n",
    "bow17 = all_bow['combined_wordle_google']\n",
    "bowwordle = all_bow['all_wordle']\n",
    "bowcur = all_bow['wordle_curated']\n",
    "\n",
    "bow17lfl = calc_lfl(wl.index, bow17, colname='all')\n",
    "bowwordlelfl = calc_lfl(wl.loc[wl['in_wordle']].index, bowwordle, colname='wordle')\n",
    "bowcurlfl = calc_lfl(wl.loc[wl['in_wordle_curated']].index, bowcur, colname='curated')\n",
    "\n",
    "lfl = pd.concat([bow17lfl, bowwordlelfl, bowcurlfl], axis=1, sort=True)\n",
    "# lfl.to_csv(r'..\\data\\processed\\letter_freq_likelihoods.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "## What sequence of words provides the highest chance of finding greens and yellows, while also not repeating any letters (i.e. maximizing grays)? \n",
    "\n",
    "### Make a function that iterates through the top 200 words with highest letter-frequency probability and finds the sequence of 5 words that has no repeats \n",
    "### If it can't find a next word with no repeat, then take the word with just one repeat (i.e. len(set(word)) == 4) \n",
    "### Then sum the calculated letter-freq probabilities of all the words \n",
    "\n",
    "### This will give a probability of finding a green but it doesn't include the helpful contribution from simply covering more letters and finding more grays. \n",
    "### To account for grays, include a \"if you have X gray letter, what % of words are excluded?\"\n",
    "\n",
    "## Important! The dataset for this optimization must use the entire wordle list, but with their letter-freq probabilities calculated from the curated list. \n",
    "\n",
    "## Then use the top 200 from curated as starting points for the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['slate', 'sauce', 'slice', 'shale', 'saute', 'share', 'sooty', 'shine',\n",
       "       'suite', 'crane',\n",
       "       ...\n",
       "       'lymph', 'jumbo', 'igloo', 'ethic', 'unzip', 'umbra', 'affix', 'ethos',\n",
       "       'inbox', 'nymph'],\n",
       "      dtype='object', name='w', length=2315)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_wordle_curated</th>\n",
       "      <th>wlh</th>\n",
       "      <th>tglp</th>\n",
       "      <th>unique_letters</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>saree</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.680346</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sooey</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.678618</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>soree</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.669546</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>saine</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.666091</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>soare</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.660043</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>saice</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.653132</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sease</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.652268</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>seare</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.644060</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>seine</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.639309</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>slane</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.639309</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       in_wordle_curated       wlh      tglp  unique_letters\n",
       "w                                                           \n",
       "saree              False  0.000037  0.680346               4\n",
       "sooey              False  0.000043  0.678618               4\n",
       "soree              False  0.000034  0.669546               4\n",
       "saine              False  0.000034  0.666091               5\n",
       "soare              False  0.000030  0.660043               5\n",
       "saice              False  0.000029  0.653132               5\n",
       "sease              False  0.000030  0.652268               3\n",
       "seare              False  0.000026  0.644060               4\n",
       "seine              False  0.000027  0.639309               4\n",
       "slane              False  0.000026  0.639309               5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = wl.loc[wl['in_wordle'], ['in_wordle_curated']]\n",
    "data['wlh'] = calc_lfl(data.index, bowcur, colname='wlh')   # wlh = Word Likelihood from letter frequency\n",
    "\n",
    "# tglp = Total Green Letter Probability\n",
    "# sum of all probabilities of finding that letter in that position \n",
    "data['tglp'] = data.index.map(lambda x: total_green_letter_prob(x, bow=bowcur)) \n",
    "curated_sorted = data.sort_values(['in_wordle_curated', 'tglp'], ascending=False)\n",
    "curated_words = curated_sorted.loc[curated_sorted['in_wordle_curated']].index\n",
    "display(curated_words)\n",
    "data = data.sort_values('tglp', ascending=False)\n",
    "data['unique_letters'] = data.index.map(lambda x: len(set(x)))\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.698488120950324, 19)\n"
     ]
    }
   ],
   "source": [
    "def calc_summed_tglp_and_repeats(sequence):\n",
    "    repeats = set()\n",
    "    pos_dict = {0: [], 1: [], 2: [], 3: [], 4: []}\n",
    "    tglp = 0\n",
    "    for word in sequence:\n",
    "        for i, letter in enumerate(word):\n",
    "            repeats.add(letter)\n",
    "            if letter not in pos_dict[i]:\n",
    "                tglp += bowcur[str(i)][letter]\n",
    "            pos_dict[i].append(letter)\n",
    "    return tglp, len(repeats)\n",
    "test_sequence = ['saint', 'cored', 'bulky', 'whomp']\n",
    "print(calc_summed_tglp_and_repeats(test_sequence))\n",
    "\n",
    "def test_word(word, pos_dict, combined_words, max_num_repeat_prev_guesses, max_num_repeats_next_guess):\n",
    "    \"\"\"\n",
    "    Optimize the guess sequence search algorithm with following conditions:\n",
    "    1. Do not repeat letters previously guessed (maximize letter information based on grays/yellows/greens)\n",
    "    2. Next gues should maximize number of new letters, up to num_repeats_allowed\n",
    "    3. If you must guess with repeat letters, do not repeat letters in the same position as before\n",
    "    \n",
    "    Do these tests with word list ranked by TGLP, and again ranked by word likelihood. \n",
    "    \n",
    "    Return True if the word being tested meets all the given criteria\n",
    "    \n",
    "    word: string\n",
    "    pos_dict: dict of index: list of letters used in this position\n",
    "    combined_words: string of combined words in sequence\n",
    "    max_num_repeat_prev_guesses: int of maximum number of repeat letters allowed from previous guesses\n",
    "    max_num_repeats_next_guess: int of maximum number of repeat letters allowed in next guess\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check for condition #3\n",
    "    for i, letter in word:\n",
    "        if letter in pos_dict[i]:\n",
    "            return False\n",
    "    \n",
    "    if not contains_any(word, combined_words, max_repeat=num_repeats_allowed) and \\\n",
    "        len(set(word) == (5-))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = data.sort_values('tglp', ascending=False).index\n",
    "tglp_list = data['tglp'].to_list()\n",
    "word_tglp_zip = zip(words, tglp_list)\n",
    "\n",
    "start_words_list = data.loc[data['in_wordle_curated']].index\n",
    "result = []\n",
    "\n",
    "# List of tuples of (number of repeats from previous guesses, number of repeats in next guess)\n",
    "# Increment each one, corresponding with a looser search tolerance\n",
    "priority_list = [(0, 0), (1, 0), (1, 1), (2, 1), (2, 2), (3, 2), (3, 3),]\n",
    "\n",
    "for i, start_word in enumerate(words[0:10]):\n",
    "    num_repeat_letters = 5 - len(set(start_word))  # Start with number of repeats in start_word \n",
    "    combined_words = start_word\n",
    "#     summed_tglp = data.loc[start_word, 'tglp']\n",
    "    sequence = [start_word]\n",
    "    \n",
    "    for seq_index in range(5):\n",
    "        for test_word, tglp in zip(words, tglp_list): \n",
    "            if not contains_any(test_word, combined_words) and (len(set(test_word)) == 5):\n",
    "                sequence.append(test_word)\n",
    "                summed_tglp += tglp\n",
    "                combined_words += test_word\n",
    "                break\n",
    "            if test_word == 'imshi':\n",
    "                # very last word was reached without finding a test_word with 5 unique letters\n",
    "                # therefore it should test for len(set(test_word)) == 4\n",
    "                for test_word, tglp in zip(words, tglp_list): \n",
    "                    if not contains_repeats(test_word, combined_words, 0) and (len(set(test_word)) >= 4):\n",
    "                        num_repeat_letters += 1\n",
    "                        sequence.append(test_word)\n",
    "                        summed_tglp += tglp\n",
    "                        combined_words += test_word\n",
    "                        break\n",
    "                        \n",
    "                if test_word == 'imshi':\n",
    "                    # very last word was reached without finding a test_word with 4 unique letters and at most 1 repeat\n",
    "                    # therefore it should test for len(set(test_word)) == 4 and at most 1 repeat\n",
    "                    for test_word, tglp in zip(words, tglp_list): \n",
    "                        if not contains_repeats(test_word, combined_words, 1) and (len(set(test_word)) >= 4):\n",
    "                            num_repeat_letters += 1\n",
    "                            sequence.append(test_word)\n",
    "                            summed_tglp += tglp\n",
    "                            combined_words += test_word\n",
    "                            break\n",
    "                        \n",
    "    result.append(sequence)\n",
    "#     result.append((start_word, summed_tglp, num_repeat_letters, sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.698488120950324, 19)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for thing in word_tglp:\n",
    "#     print(thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('saree', 1.5904967602591793, 1, ['bliny', 'chout']),\n",
       " ('sooey', 1.7442764578833694, 2, ['brant', 'child', 'jugum']),\n",
       " ('soree', 1.9416846652267816, 2, ['gaily', 'bundt', 'chack']),\n",
       " ('saine', 1.722246220302376, 1, ['borty', 'pluck', 'whiff']),\n",
       " ('soare', 1.4742980561555075, 0, ['bliny', 'dutch']),\n",
       " ('saice', 1.7624190064794816, 1, ['drony', 'flump', 'thigh']),\n",
       " ('sease', 1.9287257019438444, 3, ['crony', 'built', 'khaph']),\n",
       " ('seare', 1.5542116630669547, 1, ['bliny', 'chout']),\n",
       " ('seine', 1.8855291576673867, 2, ['coaly', 'brugh', 'tempt']),\n",
       " ('slane', 1.524406047516199, 0, ['pricy', 'fouth'])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
